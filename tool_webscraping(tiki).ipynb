{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import time\n",
        "import logging\n",
        "from typing import List, Dict"
      ],
      "metadata": {
        "id": "f9iW2wlqBQWG",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TikiScraperWithMetadata:\n",
        "    def __init__(self, base_dir=\"tiki_dataset\"):\n",
        "        self.base_dir = Path(base_dir)\n",
        "        self.base_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # Setup requests session\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "            'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8',\n",
        "            'Accept-Encoding': 'gzip, deflate, br',\n",
        "            'DNT': '1',\n",
        "            'Connection': 'keep-alive'\n",
        "        })\n",
        "\n",
        "        # Setup logging\n",
        "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "        # Danh s√°ch metadata cho t·∫•t c·∫£ s·∫£n ph·∫©m\n",
        "        self.products_metadata = []\n",
        "\n",
        "    def extract_product_info_from_url(self, product_url: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Tr√≠ch xu·∫•t th√¥ng tin s·∫£n ph·∫©m t·ª´ URL\n",
        "\n",
        "        Logic:\n",
        "        - URL: https://tiki.vn/product-p186389538.html?spid=186389559\n",
        "        - Product ID: 186389538 (t·ª´ p{ID}.html)\n",
        "        - SPID: 186389559 (t·ª´ spid parameter)\n",
        "        \"\"\"\n",
        "        product_info = {\n",
        "            \"id\": \"\",\n",
        "            \"spid\": \"\",\n",
        "            \"short_url\": product_url\n",
        "        }\n",
        "\n",
        "        # Extract Product ID t·ª´ pattern p{ID}.html\n",
        "        match = re.search(r'p(\\d+)\\.html', product_url)\n",
        "        if match:\n",
        "            product_info[\"id\"] = match.group(1)\n",
        "\n",
        "        # Extract SPID t·ª´ URL parameters\n",
        "        match = re.search(r'spid=(\\d+)', product_url)\n",
        "        if match:\n",
        "            product_info[\"spid\"] = match.group(1)\n",
        "\n",
        "        # N·∫øu kh√¥ng c√≥ SPID, d√πng Product ID\n",
        "        if not product_info[\"spid\"]:\n",
        "            product_info[\"spid\"] = product_info[\"id\"]\n",
        "\n",
        "        self.logger.debug(f\"Extracted: ID={product_info['id']}, SPID={product_info['spid']}\")\n",
        "        return product_info\n",
        "\n",
        "    def scrape_product_details(self, product_url: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Scrape chi ti·∫øt s·∫£n ph·∫©m: t√™n, ·∫£nh, v.v.\n",
        "\n",
        "        Returns:\n",
        "        {\n",
        "            \"id\": \"186389538\",\n",
        "            \"name\": \"ƒë·ªì b·ªô m·∫∑c nh√†\",\n",
        "            \"short_url\": \"https://tiki.vn/product-p186389538.html?spid=186389559\",\n",
        "            \"annotations\": \"\",\n",
        "            \"images\": [\"186389538_01.png\", \"186389538_02.png\", ...]\n",
        "        }\n",
        "        \"\"\"\n",
        "        self.logger.info(f\" Scraping product: {product_url}\")\n",
        "\n",
        "        # Extract basic info t·ª´ URL\n",
        "        product_info = self.extract_product_info_from_url(product_url)\n",
        "\n",
        "        # Kh·ªüi t·∫°o structure\n",
        "        product_data = {\n",
        "            \"id\": product_info[\"id\"],\n",
        "            \"name\": \"\",\n",
        "            \"short_url\": product_info[\"short_url\"],\n",
        "            \"annotations\": \"\",\n",
        "            \"images\": []\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = self.session.get(product_url, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # EXTRACT T√äN S·∫¢N PH·∫®M\n",
        "            product_name = self.extract_product_name(soup)\n",
        "            product_data[\"name\"] = product_name\n",
        "\n",
        "            # EXTRACT ·∫¢NH S·∫¢N PH·∫®M\n",
        "            image_urls = self.extract_product_images(soup, product_url)\n",
        "\n",
        "            # T·∫†O TH∆Ø M·ª§C S·∫¢N PH·∫®M\n",
        "            product_dir = self.create_product_directory(product_info[\"id\"])\n",
        "\n",
        "            # DOWNLOAD V√Ä L∆ØU ·∫¢NH\n",
        "            saved_images = self.download_and_save_images(\n",
        "                image_urls,\n",
        "                product_dir,\n",
        "                product_info[\"id\"]\n",
        "            )\n",
        "\n",
        "            product_data[\"images\"] = saved_images\n",
        "\n",
        "            self.logger.info(f\"‚úÖ Scraped product {product_info['id']}: {len(saved_images)} images\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"‚ùå Error scraping {product_url}: {e}\")\n",
        "\n",
        "        return product_data\n",
        "\n",
        "    def extract_product_name(self, soup: BeautifulSoup) -> str:\n",
        "        \"\"\"\n",
        "        Extract t√™n s·∫£n ph·∫©m t·ª´ HTML\n",
        "\n",
        "        Logic: T√¨m theo th·ª© t·ª± ∆∞u ti√™n\n",
        "        1. h1 v·ªõi data-view-id c·ª• th·ªÉ\n",
        "        2. h1.title\n",
        "        3. .product-name\n",
        "        4. h1 ƒë·∫ßu ti√™n\n",
        "        \"\"\"\n",
        "        name_selectors = [\n",
        "            'h1[data-view-id=\"pdp_details_view_product_title\"]',\n",
        "            'h1.title',\n",
        "            '.product-name h1',\n",
        "            '.product-title',\n",
        "            'h1'\n",
        "        ]\n",
        "\n",
        "        for selector in name_selectors:\n",
        "            element = soup.select_one(selector)\n",
        "            if element:\n",
        "                name = element.get_text().strip()\n",
        "                if name:\n",
        "                    self.logger.debug(f\"Found name via {selector}: {name[:50]}...\")\n",
        "                    return name\n",
        "\n",
        "        self.logger.warning(\"Could not extract product name\")\n",
        "        return \"Unknown Product\"\n",
        "\n",
        "    def extract_product_images(self, soup: BeautifulSoup, base_url: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Extract t·∫•t c·∫£ ·∫£nh s·∫£n ph·∫©m t·ª´ HTML\n",
        "\n",
        "        Logic:\n",
        "        1. T√¨m ·∫£nh ch√≠nh trong gallery\n",
        "        2. T√¨m ·∫£nh trong slider/carousel\n",
        "        3. T√¨m t·∫•t c·∫£ ·∫£nh c√≥ domain tikicdn\n",
        "        4. Upgrade th√†nh ·∫£nh ch·∫•t l∆∞·ª£ng cao\n",
        "        \"\"\"\n",
        "        image_urls = set()\n",
        "\n",
        "        # Method 1: ·∫¢nh ch√≠nh trong product gallery\n",
        "        main_img_selectors = [\n",
        "            'img[data-view-id=\"pdp_details_view_product_image\"]',\n",
        "            '.product-image img',\n",
        "            '.gallery-image img',\n",
        "            '.image-gallery img'\n",
        "        ]\n",
        "\n",
        "        for selector in main_img_selectors:\n",
        "            images = soup.select(selector)\n",
        "            for img in images:\n",
        "                src = img.get('src') or img.get('data-src') or img.get('data-original')\n",
        "                if src:\n",
        "                    image_urls.add(src)\n",
        "\n",
        "        # Method 2: ·∫¢nh trong slider/carousel\n",
        "        slider_selectors = [\n",
        "            '.slider img',\n",
        "            '.carousel img',\n",
        "            '.product-gallery img',\n",
        "            '.thumbnail img'\n",
        "        ]\n",
        "\n",
        "        for selector in slider_selectors:\n",
        "            images = soup.select(selector)\n",
        "            for img in images:\n",
        "                src = img.get('src') or img.get('data-src')\n",
        "                if src:\n",
        "                    image_urls.add(src)\n",
        "\n",
        "        # Method 3: T·∫•t c·∫£ ·∫£nh c√≥ domain Tiki\n",
        "        all_images = soup.find_all('img')\n",
        "        for img in all_images:\n",
        "            src = img.get('src') or img.get('data-src')\n",
        "            if src and ('tikicdn.com' in src or 'tiki.vn' in src):\n",
        "                image_urls.add(src)\n",
        "\n",
        "        # X·ª≠ l√Ω URLs\n",
        "        processed_urls = []\n",
        "        for url in image_urls:\n",
        "            # Convert to full URL\n",
        "            if url.startswith('//'):\n",
        "                url = 'https:' + url\n",
        "            elif not url.startswith('http'):\n",
        "                url = urljoin(base_url, url)\n",
        "\n",
        "            # Upgrade to high quality\n",
        "            if 'cache.tikicdn.com' in url:\n",
        "                # Thay ƒë·ªïi k√≠ch th∆∞·ªõc th√†nh 1200x1200 ho·∫∑c g·ª° b·ªè ƒë·ªÉ l·∫•y ·∫£nh g·ªëc\n",
        "                url = re.sub(r'/cache/\\d+x\\d+/', '/cache/1200x1200/', url)\n",
        "\n",
        "            # L·ªçc ·∫£nh c√≥ k√≠ch th∆∞·ªõc h·ª£p l√Ω (lo·∫°i b·ªè icon, logo nh·ªè)\n",
        "            if not any(x in url.lower() for x in ['icon', 'logo', 'badge', 'thumb']):\n",
        "                processed_urls.append(url)\n",
        "\n",
        "        # Lo·∫°i b·ªè duplicate v√† gi·ªõi h·∫°n s·ªë l∆∞·ª£ng\n",
        "        unique_urls = list(dict.fromkeys(processed_urls))[:10]  # Max 10 ·∫£nh\n",
        "\n",
        "        self.logger.debug(f\"Found {len(unique_urls)} unique images\")\n",
        "        return unique_urls\n",
        "\n",
        "    def create_product_directory(self, product_id: str) -> Path:\n",
        "        \"\"\"\n",
        "        T·∫°o th∆∞ m·ª•c cho s·∫£n ph·∫©m\n",
        "        Structure: base_dir/product_id/\n",
        "        \"\"\"\n",
        "        product_dir = self.base_dir / str(product_id)\n",
        "        product_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        self.logger.debug(f\"Created directory: {product_dir}\")\n",
        "        return product_dir\n",
        "\n",
        "    def download_and_save_images(self, image_urls: List[str],\n",
        "                                product_dir: Path, product_id: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Download v√† l∆∞u ·∫£nh v·ªõi t√™n c√≥ th·ª© t·ª±\n",
        "\n",
        "        Returns: List t√™n file ƒë√£ l∆∞u\n",
        "        Format: {product_id}_01.png, {product_id}_02.png, ...\n",
        "        \"\"\"\n",
        "        saved_images = []\n",
        "\n",
        "        for i, image_url in enumerate(image_urls, 1):\n",
        "            try:\n",
        "                # T·∫°o t√™n file v·ªõi format chu·∫©n\n",
        "                filename = f\"{product_id}_{i:02d}.png\"\n",
        "                filepath = product_dir / filename\n",
        "\n",
        "                # Skip n·∫øu file ƒë√£ t·ªìn t·∫°i\n",
        "                if filepath.exists():\n",
        "                    self.logger.debug(f\"File exists, skipping: {filename}\")\n",
        "                    saved_images.append(filename)\n",
        "                    continue\n",
        "\n",
        "                # Download image\n",
        "                self.logger.info(f\"‚¨áÔ∏è Downloading: {filename}\")\n",
        "\n",
        "                response = self.session.get(image_url, stream=True, timeout=30)\n",
        "                response.raise_for_status()\n",
        "\n",
        "                # Ki·ªÉm tra content type\n",
        "                content_type = response.headers.get('content-type', '')\n",
        "                if not content_type.startswith('image/'):\n",
        "                    self.logger.warning(f\"Not an image: {image_url}\")\n",
        "                    continue\n",
        "\n",
        "                # L∆∞u file\n",
        "                with open(filepath, 'wb') as f:\n",
        "                    for chunk in response.iter_content(chunk_size=8192):\n",
        "                        if chunk:\n",
        "                            f.write(chunk)\n",
        "\n",
        "                # Ki·ªÉm tra file size (lo·∫°i b·ªè ·∫£nh qu√° nh·ªè)\n",
        "                if filepath.stat().st_size < 8000:  # < 1KB\n",
        "                    filepath.unlink()  # X√≥a file\n",
        "                    self.logger.warning(f\"Image too small, deleted: {filename}\")\n",
        "                    continue\n",
        "\n",
        "                saved_images.append(filename)\n",
        "                self.logger.info(f\"‚úÖ Saved: {filename}\")\n",
        "\n",
        "                # Delay gi·ªØa c√°c download\n",
        "                time.sleep(0.5)\n",
        "\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"‚ùå Error downloading {image_url}: {e}\")\n",
        "                continue\n",
        "\n",
        "        return saved_images\n",
        "\n",
        "    def scrape_category_products(self, category_url: str, max_pages: int = 5) -> List[str]:\n",
        "        \"\"\"\n",
        "        L·∫•y danh s√°ch URL s·∫£n ph·∫©m t·ª´ category\n",
        "\n",
        "        Returns: List c√°c product URLs\n",
        "        \"\"\"\n",
        "        self.logger.info(f\"üîç Scraping category: {category_url}\")\n",
        "\n",
        "        all_product_urls = []\n",
        "\n",
        "        for page in range(1, max_pages + 1):\n",
        "            page_url = f\"{category_url}?page={page}\"\n",
        "            self.logger.info(f\" Page {page}: {page_url}\")\n",
        "\n",
        "            try:\n",
        "                response = self.session.get(page_url, timeout=10)\n",
        "                response.raise_for_status()\n",
        "                soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "                # T√¨m product links\n",
        "                product_links = []\n",
        "\n",
        "                # Method 1: Link c√≥ data-view-id\n",
        "                links = soup.find_all('a', {'data-view-id': 'pdp_main'})\n",
        "                for link in links:\n",
        "                    href = link.get('href')\n",
        "                    if href:\n",
        "                        product_links.append(href)\n",
        "\n",
        "                # Method 2: Link c√≥ pattern p{ID}.html\n",
        "                if not product_links:\n",
        "                    links = soup.find_all('a', href=re.compile(r'.*p\\d+\\.html'))\n",
        "                    for link in links:\n",
        "                        href = link.get('href')\n",
        "                        if href:\n",
        "                            product_links.append(href)\n",
        "\n",
        "                # Convert to full URLs\n",
        "                for href in product_links:\n",
        "                    if href.startswith('/'):\n",
        "                        full_url = f\"https://tiki.vn{href}\"\n",
        "                    else:\n",
        "                        full_url = href\n",
        "\n",
        "                    if full_url not in all_product_urls:\n",
        "                        all_product_urls.append(full_url)\n",
        "\n",
        "                self.logger.info(f\"Found {len(product_links)} products on page {page}\")\n",
        "\n",
        "                # Delay gi·ªØa c√°c trang\n",
        "                time.sleep(1)\n",
        "\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error scraping page {page}: {e}\")\n",
        "                continue\n",
        "\n",
        "        self.logger.info(f\"Total products found: {len(all_product_urls)}\")\n",
        "        return all_product_urls\n",
        "\n",
        "    def scrape_full_category(self, category_url: str, max_pages: int = 5,\n",
        "                           max_products: int = 50) -> str:\n",
        "        \"\"\"\n",
        "        Scrape to√†n b·ªô category v√† t·∫°o JSON metadata\n",
        "\n",
        "        Returns: Path to JSON file\n",
        "        \"\"\"\n",
        "        self.logger.info(f\" Starting full category scrape: {category_url}\")\n",
        "\n",
        "        # Reset metadata\n",
        "        self.products_metadata = []\n",
        "\n",
        "        # L·∫•y danh s√°ch s·∫£n ph·∫©m\n",
        "        product_urls = self.scrape_category_products(category_url, max_pages)\n",
        "\n",
        "        # Gi·ªõi h·∫°n s·ªë s·∫£n ph·∫©m\n",
        "        if len(product_urls) > max_products:\n",
        "            product_urls = product_urls[:max_products]\n",
        "            self.logger.info(f\"Limited to {max_products} products\")\n",
        "\n",
        "        # Scrape t·ª´ng s·∫£n ph·∫©m\n",
        "        for i, product_url in enumerate(product_urls, 1):\n",
        "            self.logger.info(f\"üîÑ Processing product {i}/{len(product_urls)}\")\n",
        "\n",
        "            product_data = self.scrape_product_details(product_url)\n",
        "\n",
        "            # Ch·ªâ th√™m v√†o metadata n·∫øu c√≥ ·∫£nh\n",
        "            if product_data[\"images\"]:\n",
        "                self.products_metadata.append(product_data)\n",
        "\n",
        "            # Delay gi·ªØa c√°c s·∫£n ph·∫©m\n",
        "            time.sleep(1)\n",
        "\n",
        "        # L∆∞u JSON metadata\n",
        "        json_file = self.save_metadata_json()\n",
        "\n",
        "        self.logger.info(f\" Completed! Total products: {len(self.products_metadata)}\")\n",
        "        self.logger.info(f\" JSON metadata saved: {json_file}\")\n",
        "\n",
        "        return json_file\n",
        "\n",
        "    def save_metadata_json(self) -> str:\n",
        "        \"\"\"\n",
        "        L∆∞u metadata th√†nh file JSON (gi·ªëng nh∆∞ file c·ªßa anh b·∫°n)\n",
        "        \"\"\"\n",
        "        json_file = self.base_dir / \"products_metadata.json\"\n",
        "\n",
        "        with open(json_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.products_metadata, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "        # T·∫°o summary\n",
        "        total_products = len(self.products_metadata)\n",
        "        total_images = sum(len(p[\"images\"]) for p in self.products_metadata)\n",
        "\n",
        "        summary = {\n",
        "            \"total_products\": total_products,\n",
        "            \"total_images\": total_images,\n",
        "            \"products\": self.products_metadata\n",
        "        }\n",
        "\n",
        "        summary_file = self.base_dir / \"summary.json\"\n",
        "        with open(summary_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(summary, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "        return str(json_file)\n",
        "\n",
        "# DEMO: S·ª≠ d·ª•ng gi·ªëng nh∆∞ script c·ªßa anh b·∫°n\n",
        "def main():\n",
        "    scraper = TikiScraperWithMetadata(\"tiki_fashion_dataset\")\n",
        "\n",
        "    # Scrape th·ªùi trang n·ªØ (gi·ªëng data c·ªßa anh b·∫°n)\n",
        "    category_url = \"https://tiki.vn/nha-sach-tiki/c8322\"\n",
        "\n",
        "    json_file = scraper.scrape_full_category(\n",
        "        category_url=category_url,\n",
        "        max_pages=3,\n",
        "        max_products=25\n",
        "    )\n",
        "\n",
        "    print(f\" Ho√†n th√†nh! JSON file: {json_file}\")\n",
        "\n",
        "    # In th·ªëng k√™\n",
        "    print(f\"  Th·ªëng k√™:\")\n",
        "    print(f\"   - T·ªïng s·∫£n ph·∫©m: {len(scraper.products_metadata)}\")\n",
        "    print(f\"   - T·ªïng ·∫£nh: {sum(len(p['images']) for p in scraper.products_metadata)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "iQRwhl04GylU",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/tiki_books_dataset"
      ],
      "metadata": {
        "id": "u1piGrcgVEE7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b2d90f6-44af-4c49-8b4e-8da95375ca61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/tiki_books_dataset': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/tiki_fashion_dataset/"
      ],
      "metadata": {
        "id": "4olnyFqz4Pqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nzmmC-r94TGq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}